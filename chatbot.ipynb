{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1. Build our Kernel\n",
    "\n",
    "First step is to build the kernel with an LLM. We could use Azure OpenAI, OpenAI model and HuggingFace community Models. We are going to use OpenAI because it has a good performance. All we need is to head to <platform.openai.com> and signup for a free account (it should give you some credits to start building your first app). create organization, copy its ID, head out to API-Keys and create a new secret key (Don't share it with anyone, thats why it is called a secret key) and place them in the code snippet below which you have to run in jupyter notebook using .NET interactive Kernel.\n",
    "\n",
    "> Note for the chatModel we could use gpt-4 or even newer preview models but it will cost more besides gpt-3.5-turbo is already better than most of other LLM models available today. but if you want the best experience and cost is not an issue by all means choose gpt-4 or gpt-4-1106-preview turbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.SemanticKernel, 1.0.0-rc3</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// this is still release candidate not the first release but it will most likely be identical to the first release\n",
    "#r \"nuget: Microsoft.SemanticKernel, 1.0.0-rc3\"\n",
    "using Microsoft.SemanticKernel;\n",
    "using Microsoft.SemanticKernel.Connectors.AI.OpenAI;\n",
    "\n",
    "var builder = new KernelBuilder();\n",
    "string organizationID = \"org-xxxxxxxxxx\"; //TODO: regional endpoint organization id\n",
    "string openAIKey = \"sk-xxxxxxxxxx\"; //TODO: replace this with your key\n",
    "string chatModel = \"gpt-3.5-turbo\"; //we can use gpt-4 or ony other model\n",
    "\n",
    "// We can Use Azure OpenAI which supports regional endpoints which meets GDPR and other regulations\n",
    "builder.AddOpenAIChatCompletion(chatModel, openAIKey, organizationID);\n",
    "var kernel = builder.Build();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2. Build Our Chat Function\n",
    "\n",
    "Now let's build simple chat function with minimum settings. All we need is the prompt with an argument for user input, execution settings with setting parameters like MaxTokens, Temprature and TopP and if you don't know what these are, there are some good articles online explaining it like [this one](https://peterchng.com/blog/2023/05/02/token-selection-strategies-top-k-top-p-and-temperature/)\n",
    "\n",
    "We build our function setting our function from prompt (we could save our prompt in a directory as a text file and use it just as easily as this one)\n",
    "\n",
    "Pay attention to the order of the code. We get user's input (hardcoded here) and then add it as kernelargument and then invoke our function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi, I'm looking for book suggestions\n",
      "ChatBot: Hi! I'd be happy to help you with book suggestions. What genre or type of books are you interested in?\n"
     ]
    }
   ],
   "source": [
    "const string skPrompt = @\"\n",
    "ChatBot can have a conversation with you about any topic.\n",
    "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
    "\n",
    "User: {{$userInput}}\n",
    "ChatBot:\";\n",
    "\n",
    "var executionSettings = new OpenAIPromptExecutionSettings \n",
    "{\n",
    "    MaxTokens = 2000,\n",
    "    Temperature = 0.7,\n",
    "    TopP = 0.5\n",
    "};\n",
    "\n",
    "var chatFunction = kernel.CreateFunctionFromPrompt(skPrompt, executionSettings);\n",
    "var arguments = new KernelArguments();\n",
    "\n",
    "\n",
    "\n",
    "var userInput = \"Hi, I'm looking for book suggestions\";\n",
    "arguments[\"userInput\"] = userInput;\n",
    "\n",
    "string bot_answer = await chatFunction.InvokeAsync<string>(kernel, arguments);\n",
    "\n",
    "Console.WriteLine(\"User: \" + userInput);\n",
    "Console.WriteLine(\"ChatBot: \" + bot_answer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Function invokation is done async but the chat response will apear all in once which is both slower and is not similar to Chat-GPT experience. for fixing that, there is an option for invoking function using StreamingAsync that returns IAsyncEnumerable response which will enable streaming for awesome user experience. so the code will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I'd be happy to help you with book suggestions. What genre or type of books are you interested in?"
     ]
    }
   ],
   "source": [
    "using System.Threading;\n",
    "IAsyncEnumerable<string> stream_answer = chatFunction.InvokeStreamingAsync<string>(kernel,arguments,CancellationToken.None);\n",
    "await foreach (var answer in stream_answer)\n",
    "{\n",
    "    Console.Write(answer);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3. Include History and make it a conversation chat-bot \n",
    "\n",
    "So far we have built a simple chat function but we want to make it a back and forth conversation chat app. for that we are missing a key element which is to keep the history of our chat because the function is stateless and don't know the context of the user input unless you give the context in the form of chat history to it. for that, we need to add another argument for chat history and update it on every turn.Code will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "const string skPrompt = @\"\n",
    "ChatBot can have a conversation with you about any topic.\n",
    "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
    "\n",
    "{{$history}}\n",
    "User: {{$userInput}}\n",
    "ChatBot:\";\n",
    "\n",
    "var executionSettings = new OpenAIPromptExecutionSettings \n",
    "{\n",
    "    MaxTokens = 2000,\n",
    "    Temperature = 0.7,\n",
    "    TopP = 0.5\n",
    "};\n",
    "\n",
    "var chatFunction = kernel.CreateFunctionFromPrompt(skPrompt, executionSettings);\n",
    "var arguments = new KernelArguments();\n",
    "string history = string.Empty;\n",
    "while(true)\n",
    "{\n",
    "    var userInput = await Microsoft.DotNet.Interactive.Kernel.GetInputAsync(\"User: \");\n",
    "    arguments[\"history\"] = history;\n",
    "    arguments[\"userInput\"] = userInput;\n",
    "\n",
    "    var bot_answer = await chatFunction.InvokeAsync(kernel, arguments);\n",
    "    history += $\"User: {userInput}\\nChatBot: {bot_answer}\\n\\n\";\n",
    "    Console.WriteLine(\"User: \" + userInput);\n",
    "    Console.WriteLine(\"ChatBot: \" + bot_answer);\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4. Use embeddings to avoid hitting the token limit\n",
    "\n",
    "So far we have built our simple chat-bot which is great but you will find out depending on the size of chat entries, response size and the token limit of the model that you are using, after only few turns you will hit the input token Limit on your LLM. Long story short, you can't just keep building a history and keep passing it as an input parameter to the kernel function.\n",
    "\n",
    "To fix this, we should use another technique or concept in ML called the embeddings. You can think of embeddings as a mathemtical representations of values or objects like text, images, and audio that are designed to be consumed by machine learning models and semantic search algorithms. They translate objects like these into a mathematical form according to the factors or traits each one may or may not have, and the categories they belong to. \n",
    "\n",
    "Essentially, embeddings enable machine learning models to find similar objects. Given a photo or a document, a machine learning model that uses embeddings could find a similar photo or document. Since embeddings make it possible for computers to understand the relationships between words and other objects, they are foundational for artificial intelligence (AI).\n",
    "\n",
    "Technically, embeddings are vectors created by machine learning models for the purpose of capturing meaningful data about each object. you can learn more about it by looking it up or read blog posts about it like this [definitive guide to embeddings](https://www.featureform.com/post/the-definitive-guide-to-embeddings)\n",
    "\n",
    "Thankfully, the folks who have created SemanticKernel have thought of this and created a cool mechanism to incluse these embeddings in our function calls called `Memories`. We used kernel argument to fill the prompt with a `history` that continuously got populated as we chatted with the bot. Let's use memory instead! For that we need to narrow our scope which in case of our chatbot, Stars-AI we already know we are building a career coach ChatBot. So we need to gather some relavant facts about users professional goals. This is done by using the `TextMemoryPlugin` which exposes the `recall` native function.\n",
    "`recall` takes an input ask and performs a similarity search on the contents that have been embedded in the Memory Store. By default, `recall` returns the most relevant memory. so here is roughly almost the exact same code I used for Stars-AI in this website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "using Microsoft.SemanticKernel.Memory;\n",
    "using Microsoft.SemanticKernel.Plugins.Memory;\n",
    "#pragma warning disable SKEXP0011, SKEXP0003, SKEXP0052 //memory builder is experimental\n",
    "\n",
    "const string MemoryCollectionName = \"chatHistory\"; \n",
    "var memoryBuilder = new MemoryBuilder();\n",
    "\n",
    "//Lets use another OpenAI's model for text embedding\n",
    "string embeddingModel = \"text-embedding-ada-002\"; // this is the only model that supports text embedding\n",
    "memoryBuilder.WithOpenAITextEmbeddingGeneration(embeddingModel, openAIKey, organizationID);\n",
    "// For now we will use In-memory store\n",
    "memoryBuilder.WithMemoryStore(new VolatileMemoryStore());\n",
    "var memory = memoryBuilder.Build();\n",
    "\n",
    "const string skPrompt = @\"\n",
    "        You are StarsAI, a very polite and professional chat-bot, and you are chatting with \"\"{{$user}}\"\" who is the user. You are a Career and life coach and an expert teacher in different topics giving people advice to get them from their current level to the point that they can be hired as a professional or where they want to be. Don't answer random questions outside of learning and career topics. Just act as a responsible and patient teacher and career coach to help people with what they need to learn or do to advance their careers. \n",
    "        Consider following facts, goals and personal information about \"\"{{$user}}\"\":\n",
    "        - {{$fact0}} {{recall $fact0}}\n",
    "        - {{$fact1}} {{recall $fact1}}\n",
    "        - {{$fact2}} {{recall $fact2}}\n",
    "        - {{$fact3}} {{recall $fact3}}\n",
    "        - {{$fact4}} {{recall $fact4}}\n",
    "        - {{$fact5}} {{recall $fact5}}\n",
    "        \n",
    "        If the conversation has not started yet, start by prompting: \"\"Welcome to StarsAI \"\"{{$user}}\"\", Tell me a little bit about yourself, what is your education level and what are your career goals?\"\" but don't show that if the conversation has started.\n",
    "        Always consider what has been asked before and don't ask the same question. consider User answers for asking next question. \n",
    "        After getting the input, layout a study guide and steps, online or university courses or even certificates user need to take or pass to have the best shot to get hired as a professional with the best salary possible. Always suggest shortest and most affordable options for learning, for example taking online courses. Only suggest getting a university degree if it is absolutely necessary for the job function\n",
    "\n",
    "        \n",
    "        User: {{$userInput}}\n",
    "        StarsAI:\";\n",
    "var executionSettings = new OpenAIPromptExecutionSettings \n",
    "{\n",
    "    MaxTokens = 3000,\n",
    "    Temperature = 0.8,\n",
    "    TopP = 0.5\n",
    "};\n",
    "chatFunction = kernel.CreateFunctionFromPrompt(skPrompt, executionSettings);\n",
    "\n",
    "arguments[\"fact0\"] = \"conversation started:\";\n",
    "arguments[\"fact1\"] = \"education level:\";\n",
    "arguments[\"fact2\"] = \"career goal:\";\n",
    "arguments[\"fact3\"] = \"work history:\";\n",
    "arguments[\"fact4\"] = \"desired job:\";\n",
    "arguments[\"fact5\"] = \"desired salary:\";\n",
    "arguments[TextMemoryPlugin.CollectionParam] = MemoryCollectionName;\n",
    "arguments[TextMemoryPlugin.LimitParam] = \"2\"; //how many memories to recall for a specific fact\n",
    "arguments[TextMemoryPlugin.RelevanceParam] = \"0.6\"; //measure of the relevance score from 0.0 to 1.0, where 1.0 means a perfect match.\n",
    "// We need to import the plugin to the kernel\n",
    "// do this only once either through constructor or transient dependency injection\n",
    "kernel.ImportPluginFromObject(new TextMemoryPlugin(memory));\n",
    "\n",
    "// ...\n",
    "// Later in the code\n",
    "// Chat Object from Database\n",
    "public class Chat\n",
    "{\n",
    "    public string Id { get; set; } = Guid.NewGuid().ToString();\n",
    "    public string Message { get; set; }\n",
    "    public string Role { get; set; }\n",
    "    public string UserName { get; set; }\n",
    "    public DateTime CreationDate { get; set; }\n",
    "}\n",
    "var msg = new Chat { Message = \"chat message\", Role = \"User\", UserName = \"John Doe\", CreationDate = DateTime.UtcNow };\n",
    "await memory.SaveInformationAsync(collection: MemoryCollectionName, id: msg.Id, text: msg.Message);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5. Use vector database instead of In-Memory VolatileMemoryStore\n",
    "\n",
    "We are almost there. So far, everything works great and we will not hit the Token Limit ever again. However, there is just one problem and that is our embeddings now live in the memory. we can rebuild them no problem (although it will hurt performance but doable) but the main issue is that as our application grow and so many people start using it, the server memory will hit its limit very soon so we need to seek a better solution for this and permanently store our embeddings and that is nowhere but the Vector Databases because embeddings are vectors.\n",
    "\n",
    "Again, Semantic Kernel got our back and it support a wide variety of databases. For Starspak, we are already using PostgreSQL DB and it turns out PostgreSQL support this through `VECTOR` extension. why not using it for this purpose too? All you have to do is to enable this extension and call `CREATE EXTENSION VECTOR;`\n",
    "\n",
    "> Note, \"Azure Cosmos DB for PostgreSQL\" uses `SELECT CREATE_EXTENSION('vector');` to enable the extension.\n",
    "\n",
    "Here is the code modification you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.SemanticKernel.Connectors.Memory.Postgres, 1.0.0-rc3</span></li><li><span>Pgvector, 0.2.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel.Connectors.Memory.Postgres, 1.0.0-rc3\"\n",
    "#r \"nuget: Pgvector, 0.2.0\"\n",
    "#pragma warning disable SKEXP0032, SKEXP0052 //memory builder is experimental\n",
    "using Microsoft.SemanticKernel.Connectors.Memory.Postgres;\n",
    "using Npgsql;\n",
    "using Pgvector;\n",
    "\n",
    "// Use Postgres Memory Store\n",
    "NpgsqlDataSourceBuilder dataSourceBuilder = new(\"Server=localhost;Database=db;User Id=user;Password=pw\"); //TODO: Replace with your connection string\n",
    "dataSourceBuilder.UseVector();\n",
    "NpgsqlDataSource dataSource = dataSourceBuilder.Build();\n",
    "PostgresMemoryStore memoryStore = new(dataSource,1536);\n",
    "\n",
    "memoryBuilder.WithMemoryStore(memoryStore);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
